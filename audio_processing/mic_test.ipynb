{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from pydub import AudioSegment\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import itertools\n",
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, Resize, Normalize\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Grayscale, ToTensor, Compose, Resize, InterpolationMode, Normalize, Lambda\n",
    ")\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Audio settings\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1  # Mono recording\n",
    "RATE = 16000  # Sampling rate in Hz\n",
    "CHUNK = 1024  # Buffer size\n",
    "RECORD_SECONDS = 3\n",
    "\n",
    "\n",
    "def record_audio_to_numpy():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    \n",
    "    # Open the microphone stream\n",
    "    \n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "    \n",
    "    # print(\"Recording...\")\n",
    "    frames = []\n",
    "    \n",
    "    # Read data from the stream\n",
    "    for _ in range(int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    \n",
    "    # print(\"Recording finished.\")\n",
    "    \n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    \n",
    "    # Convert recorded frames to numpy array\n",
    "    audio_array = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "    return audio_array\n",
    "\n",
    "def numpy_to_fft(samples, sr = 16000):\n",
    "    n_fft=2048 \n",
    "    win_length1 = 750 \n",
    "    hop_length=win_length1//4 \n",
    "\n",
    "    # Convert to NumPy array and normalize\n",
    "    samples = samples.astype(np.float32)/float(np.max(samples))\n",
    "\n",
    "    # Compute spectrogram using STFT\n",
    "    S = librosa.stft(samples, n_fft=n_fft,win_length = win_length1, hop_length=hop_length)  \n",
    "\n",
    "    #The number of rows in the STFT matrix D is (1 + n_fft/2).\n",
    "    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "    S0 = S_db\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2.56, 2.56), dpi=100)  # 256x256 pixels\n",
    "    librosa.display.specshow(S_db, sr=sr, n_fft=n_fft, win_length=win_length1, hop_length=hop_length, x_axis=\"time\", y_axis=\"log\", cmap=\"gray\")\n",
    "\n",
    "    # Remove axes for a clean image\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout(pad=0)\n",
    "    # print(plt.ylim())\n",
    "    plt.ylim([30,6000])\n",
    "    buf = io.BytesIO()\n",
    "    # plt.clf()\n",
    "    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0, dpi=100)\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)  # Move cursor to the start of the buffer\n",
    "\n",
    "    # Process the image from memory\n",
    "    img = Image.open(buf).convert(\"L\")  # Convert to grayscale\n",
    "    img = img.resize((256, 256))  # Resize to 256x256\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    # img_array = np.array(img, dtype=np.uint8)\n",
    "    # img_array = img_array[np.newaxis, :, :]  # Add channel dimension (1,256,256)\n",
    "    \n",
    "    # Close buffer\n",
    "    buf.close()\n",
    "    return img#Image.fromarray(img_array.squeeze())\n",
    "\n",
    "\n",
    "def fft_to_tensor(pil_image):\n",
    "   \n",
    "    image = pil_image\n",
    "\n",
    "    # Rotate 90 degrees (optional, remove if unnecessary)\n",
    "    image = image.transpose(Image.ROTATE_90)\n",
    "\n",
    "    # Ensure the image is in RGB mode (some formats might be grayscale)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    transform = Compose([\n",
    "                            ToTensor(),\n",
    "                            Grayscale(),\n",
    "                            Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "                            Normalize(mean=[0.5], std=[0.5])\n",
    "                        ])\n",
    "    \n",
    "\n",
    "    input_tensor = transform(image)  # Shape: (1, 224, 224)\n",
    "    input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension -> (1, 1, 224, 224)\n",
    "    if input_tensor.shape[1] == 1:  # Convert grayscale to RGB\n",
    "        input_tensor = input_tensor.expand(-1, 3, -1, -1)  # Shape: (1, 3, H, W)\n",
    "\n",
    "    # Apply transformations and return image with label\n",
    "    return input_tensor\n",
    "\n",
    "def evaluate(output):\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = np.array(F.softmax(output, dim=1).squeeze())  # Shape: (num_classes,)\n",
    "    threshold = 0.95\n",
    "    # Class labels\n",
    "    guess = np.argmax(probabilities)\n",
    "    class_labels = ['arabic', 'english', 'german', 'mandarin', 'spanish', 'garbage']\n",
    "\n",
    "    # Display probabilities\n",
    "    if probabilities[guess]>threshold:\n",
    "        print(class_labels[guess], round(probabilities[guess]*100, 2),\"%\")\n",
    "    else:\n",
    "        print(f\"unsure ({class_labels[guess]}, { round(probabilities[guess]*100, 2)}%\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german 95.28 %\n",
      "english 99.93 %\n",
      "english 99.99 %\n",
      "german 99.76 %\n",
      "unsure (english, 88.22%\n",
      "english 99.99 %\n",
      "english 99.33 %\n",
      "unsure (german, 87.1%\n",
      "english 98.34 %\n",
      "unsure (english, 90.85%\n",
      "english 99.93 %\n",
      "english 99.96 %\n",
      "english 99.68 %\n",
      "english 100.0 %\n",
      "unsure (english, 74.33%\n"
     ]
    }
   ],
   "source": [
    "image = None\n",
    "model_path = r\"ENDG511_Final_Project/models/model_language_wgarb.pth\"\n",
    "model = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "for i in range(15):\n",
    "    t0 = time.time()\n",
    "    array = record_audio_to_numpy()\n",
    "    dt = time.time() - t0\n",
    "    # print(\"record time: \", dt)\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "    image = numpy_to_fft(array)\n",
    "    dt = time.time() - t0\n",
    "    # print(\"process time: \", dt)\n",
    "    image.save(\"spectrogram.png\")\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "    input_tensor = fft_to_tensor(image)\n",
    "    with torch.no_grad(): output = model(input_tensor)  # Get raw logits\n",
    "    dt = time.time() - t0\n",
    "    # print(\"inference time: \", dt)\n",
    "\n",
    "    evaluate(output)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_100543000.png bad\n",
      "english_100544000.png bad\n",
      "english_100544001.png bad\n",
      "english_100545000.png bad\n"
     ]
    }
   ],
   "source": [
    "# image = None\n",
    "# model_path = r\"ENDG511_Final_Project/models/model_language_wgarb.pth\"\n",
    "# model = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "# model.eval()  # Set to evaluation mode\n",
    "\n",
    "\n",
    "# temp_file = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\audio_processing\\spectrogram_000.png\"\n",
    "file_path = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\languages\"\n",
    "for temp_file in os.listdir(file_path):\n",
    "    # print(temp_file)\n",
    "    filebad = 0\n",
    "    with Image.open(file_path+\"\\\\\"+temp_file) as img:\n",
    "        img_array = np.array(img)\n",
    "        if np.all(img_array == 0):\n",
    "            file_bad = 1\n",
    "            print(temp_file, \"bad\")\n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from functools import partial\n",
    "# from torchvision.models import MobileNetV2\n",
    "\n",
    "\n",
    "model = torch.load(r\"C:\\Git_repos\\ENDG 511\\model_language.pth\", map_location=\"cpu\", weights_only=False)\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.models.mobilenetv2.MobileNetV2'>\n",
      "Not a dict\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)  # Get raw logits\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "probabilities = F.softmax(output, dim=1).squeeze()  # Shape: (num_classes,)\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['arabic', 'english', 'german', 'mandarin', 'spanish']\n",
    "\n",
    "# Display probabilities\n",
    "for label, prob in zip(class_labels, probabilities):\n",
    "    print(f\"{label}: {prob:.4f}\")  # Format to 4 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_unused_garbage_number(folder_path, file_name):\n",
    "    pattern = re.compile(f\"{file_name}(\\\\d+)\\\\.png\")\n",
    "    used_numbers = set()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            used_numbers.add(int(match.group(1)))\n",
    "\n",
    "    if not used_numbers:\n",
    "        return 0  # Return 0 if there are no files\n",
    "\n",
    "    # Find the first missing number\n",
    "    for i in range(1, 10001):\n",
    "        if i not in used_numbers:\n",
    "            return i\n",
    "\n",
    "    return None  # If all numbers are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved:    \n",
      "\b131313131313131313141414141414141414141515151515151515151516161616161616161616171717171717171717171818181818181818181819191919191919197"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved:    \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mrecord_audio_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     image \u001b[38;5;241m=\u001b[39m numpy_to_fft(array)\n\u001b[0;32m      8\u001b[0m     number \u001b[38;5;241m=\u001b[39m find_first_unused_garbage_number(GARBAGE_FOLDER, file_name)\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mrecord_audio_to_numpy\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Read data from the stream\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(RATE \u001b[38;5;241m/\u001b[39m CHUNK \u001b[38;5;241m*\u001b[39m RECORD_SECONDS)):\n\u001b[1;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(\"Recording finished.\")\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Stop and close the stream\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\levis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GARBAGE_FOLDER = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\audio_processing\\sample_dataset\\garbage_audio\"\n",
    "GARBAGE_FOLDER = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\audio_processing\\sample_dataset\\english_extra\"\n",
    "file_name = \"english\"\n",
    "print(\"saved:    \")\n",
    "while True:\n",
    "    array = record_audio_to_numpy()\n",
    "    image = numpy_to_fft(array)\n",
    "    number = find_first_unused_garbage_number(GARBAGE_FOLDER, file_name)\n",
    "    image.save(f'{GARBAGE_FOLDER}\\\\{file_name}{number}.png')\n",
    "    image.save(f'temp.png')\n",
    "\n",
    "    print(\"\\b\\b\\b\", end=\"\")\n",
    "    print(number, end=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hihihihih1None3\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\"*5, end=\"\")\n",
    "print(\"\\b1\", end = \"None\")\n",
    "print(len(\"121\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
