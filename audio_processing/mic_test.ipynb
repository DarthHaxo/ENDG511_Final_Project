{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from pydub import AudioSegment\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import itertools\n",
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, Resize, Normalize\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Grayscale, ToTensor, Compose, Resize, InterpolationMode, Normalize, Lambda\n",
    ")\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Audio settings\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1  # Mono recording\n",
    "RATE = 16000  # Sampling rate in Hz\n",
    "CHUNK = 1024  # Buffer size\n",
    "RECORD_SECONDS = 3\n",
    "\n",
    "\n",
    "def record_audio_to_numpy():\n",
    "    audio = pyaudio.PyAudio()\n",
    "    \n",
    "    # Open the microphone stream\n",
    "    \n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "    \n",
    "    # print(\"Recording...\")\n",
    "    frames = []\n",
    "    \n",
    "    # Read data from the stream\n",
    "    for _ in range(int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    \n",
    "    # print(\"Recording finished.\")\n",
    "    \n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    \n",
    "    # Convert recorded frames to numpy array\n",
    "    audio_array = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "    return audio_array\n",
    "\n",
    "def numpy_to_fft(samples, sr = 16000):\n",
    "    n_fft=2048 \n",
    "    win_length1 = 750 \n",
    "    hop_length=win_length1//4 \n",
    "\n",
    "    # Convert to NumPy array and normalize\n",
    "    samples = samples.astype(np.float32)/float(np.max(samples))\n",
    "\n",
    "    # Compute spectrogram using STFT\n",
    "    S = librosa.stft(samples, n_fft=n_fft,win_length = win_length1, hop_length=hop_length)  \n",
    "\n",
    "    #The number of rows in the STFT matrix D is (1 + n_fft/2).\n",
    "    S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "    S0 = S_db\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2.56, 2.56), dpi=100)  # 256x256 pixels\n",
    "    librosa.display.specshow(S_db, sr=sr, n_fft=n_fft, win_length=win_length1, hop_length=hop_length, x_axis=\"time\", y_axis=\"log\", cmap=\"gray\")\n",
    "\n",
    "    # Remove axes for a clean image\n",
    "    ax.set_axis_off()\n",
    "    plt.tight_layout(pad=0)\n",
    "    # print(plt.ylim())\n",
    "    plt.ylim([30,6000])\n",
    "    buf = io.BytesIO()\n",
    "    # plt.clf()\n",
    "    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\", pad_inches=0, dpi=100)\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)  # Move cursor to the start of the buffer\n",
    "\n",
    "    # Process the image from memory\n",
    "    img = Image.open(buf).convert(\"L\")  # Convert to grayscale\n",
    "    img = img.resize((256, 256))  # Resize to 256x256\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    # img_array = np.array(img, dtype=np.uint8)\n",
    "    # img_array = img_array[np.newaxis, :, :]  # Add channel dimension (1,256,256)\n",
    "    \n",
    "    # Close buffer\n",
    "    buf.close()\n",
    "    return img#Image.fromarray(img_array.squeeze())\n",
    "\n",
    "\n",
    "def fft_to_tensor(pil_image):\n",
    "   \n",
    "    image = pil_image\n",
    "\n",
    "    # Rotate 90 degrees (optional, remove if unnecessary)\n",
    "    image = image.transpose(Image.ROTATE_90)\n",
    "\n",
    "    # Ensure the image is in RGB mode (some formats might be grayscale)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    transform = Compose([\n",
    "                            ToTensor(),\n",
    "                            Grayscale(),\n",
    "                            Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "                            Normalize(mean=[0.5], std=[0.5])\n",
    "                        ])\n",
    "    \n",
    "\n",
    "    input_tensor = transform(image)  # Shape: (1, 224, 224)\n",
    "    input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension -> (1, 1, 224, 224)\n",
    "    if input_tensor.shape[1] == 1:  # Convert grayscale to RGB\n",
    "        input_tensor = input_tensor.expand(-1, 3, -1, -1)  # Shape: (1, 3, H, W)\n",
    "\n",
    "    # Apply transformations and return image with label\n",
    "    return input_tensor\n",
    "\n",
    "def evaluate(output, threshold = .9):\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = np.array(F.softmax(output, dim=1).squeeze())  # Shape: (num_classes,)\n",
    "    # Class labels\n",
    "    guess = np.argmax(probabilities)\n",
    "    class_labels = ['arabic', 'english', 'german', 'mandarin', 'spanish', 'garbage', 'french']\n",
    "\n",
    "    # Display probabilities\n",
    "    language = class_labels[guess]\n",
    "    confidence = probabilities[guess]\n",
    "\n",
    "    # if np.random.rand()<.2: language = \"english!\"\n",
    "    if confidence>threshold:\n",
    "        return language, confidence\n",
    "    \n",
    "    return f\"low confidence({language})\", confidence\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('low confidence(garbage)', '88.935%')\n",
      "('low confidence(garbage)', '85.72%')\n",
      "('garbage', '98.134%')\n",
      "('garbage', '92.408%')\n",
      "('low confidence(garbage)', '89.807%')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow(block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# generate image and predict with model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mrecord_audio_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m numpy_to_fft(array) \u001b[38;5;66;03m#generate a pil image of the spectrogram\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m fft_to_tensor(image)\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mrecord_audio_to_numpy\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Read data from the stream\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(RATE \u001b[38;5;241m/\u001b[39m CHUNK \u001b[38;5;241m*\u001b[39m RECORD_SECONDS)):\n\u001b[1;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(\"Recording finished.\")\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Stop and close the stream\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\levis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "image = None\n",
    "model_path = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\models\\model_language_fix_data_10_epoch.pth\"\n",
    "model = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# set up the plot window\n",
    "matplotlib.use('TkAgg')  # Use a GUI backend for external window\n",
    "# plt.ion()\n",
    "# fig, ax = plt.subplots()\n",
    "# img1 = Image.open(\"spectrogram.png\")\n",
    "# im_display = ax.imshow(img1)\n",
    "# title = ax.set_title(\"Image 1\")  # Set initial title\n",
    "# plt.axis('off')  # Hide axes\n",
    "# plt.show()\n",
    "\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "img1 = Image.open(\"spectrogram.png\")\n",
    "im_display = ax.imshow(img1, animated=True)\n",
    "title = ax.set_title(\"Image 1\")\n",
    "plt.axis('off')\n",
    "plt.show(block=False)\n",
    "\n",
    "for i in range(15):\n",
    "    # generate image and predict with model\n",
    "    array = record_audio_to_numpy()\n",
    "    image = numpy_to_fft(array) #generate a pil image of the spectrogram\n",
    "    input_tensor = fft_to_tensor(image)\n",
    "    with torch.no_grad(): output = model(input_tensor) \n",
    "    language, confidence = evaluate(output)\n",
    "    text_out = language, f'{round(confidence*100, 3)}%'\n",
    "    print(text_out)\n",
    "\n",
    "    #update the plot window\n",
    "    image.save(\"spectrogram.png\")\n",
    "    img2 = Image.open(\"spectrogram.png\")\n",
    "    im_display.set_data(img2)\n",
    "    title.set_text(text_out)  # Update title\n",
    "\n",
    "    # fig.canvas.draw()\n",
    "    # fig.canvas.flush_events()\n",
    "    fig.canvas.draw_idle()  # More responsive than draw()\n",
    "    plt.pause(0.1)  # Allow GUI to update smoothly\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_100543000.png bad\n",
      "english_100544000.png bad\n",
      "english_100544001.png bad\n",
      "english_100545000.png bad\n"
     ]
    }
   ],
   "source": [
    "# image = None\n",
    "# model_path = r\"ENDG511_Final_Project/models/model_language_wgarb.pth\"\n",
    "# model = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "# model.eval()  # Set to evaluation mode\n",
    "\n",
    "\n",
    "# temp_file = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\audio_processing\\spectrogram_000.png\"\n",
    "file_path = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\languages\"\n",
    "for temp_file in os.listdir(file_path):\n",
    "    # print(temp_file)\n",
    "    filebad = 0\n",
    "    with Image.open(file_path+\"\\\\\"+temp_file) as img:\n",
    "        img_array = np.array(img)\n",
    "        if np.all(img_array == 0):\n",
    "            file_bad = 1\n",
    "            print(temp_file, \"bad\")\n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from functools import partial\n",
    "# from torchvision.models import MobileNetV2\n",
    "\n",
    "\n",
    "model = torch.load(r\"C:\\Git_repos\\ENDG 511\\model_language.pth\", map_location=\"cpu\", weights_only=False)\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.models.mobilenetv2.MobileNetV2'>\n",
      "Not a dict\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)  # Get raw logits\n",
    "\n",
    "# Apply softmax to convert logits to probabilities\n",
    "probabilities = F.softmax(output, dim=1).squeeze()  # Shape: (num_classes,)\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['arabic', 'english', 'german', 'mandarin', 'spanish']\n",
    "\n",
    "# Display probabilities\n",
    "for label, prob in zip(class_labels, probabilities):\n",
    "    print(f\"{label}: {prob:.4f}\")  # Format to 4 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_unused_garbage_number(folder_path, file_name):\n",
    "    pattern = re.compile(f\"{file_name}(\\\\d+)\\\\.png\")\n",
    "    used_numbers = set()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            used_numbers.add(int(match.group(1)))\n",
    "\n",
    "    if not used_numbers:\n",
    "        return 0  # Return 0 if there are no files\n",
    "\n",
    "    # Find the first missing number\n",
    "    for i in range(1, 10001):\n",
    "        if i not in used_numbers:\n",
    "            return i\n",
    "\n",
    "    return None  # If all numbers are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved:    \n",
      ".2.3.4.5.6.7.8.9.\n",
      "10.11.12.13.14.15.16.17.18.19.\n",
      "20.21.22.23.24.25.26.27.28.29.\n",
      "30.31.32.33.34.35.36.37.38.39.\n",
      "40.41.42.43.44.45.46.47.48.49.\n",
      "50.51.52.53.54.55.56.57.58.59.\n",
      "60.61.62.63.64.65.66.67.68.69.\n",
      "70"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mrecord_audio_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     image \u001b[38;5;241m=\u001b[39m numpy_to_fft(array)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# number = find_first_unused_garbage_number(GARBAGE_FOLDER, file_name)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m, in \u001b[0;36mrecord_audio_to_numpy\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Read data from the stream\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(RATE \u001b[38;5;241m/\u001b[39m CHUNK \u001b[38;5;241m*\u001b[39m RECORD_SECONDS)):\n\u001b[1;32m---> 23\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(\"Recording finished.\")\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Stop and close the stream\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\levis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GARBAGE_FOLDER = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\audio_processing\\sample_dataset\\garbage_audio\"\n",
    "GARBAGE_FOLDER = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\audio_processing\\sample_dataset\\english_extra\"\n",
    "GARBAGE_FOLDER = r\"C:\\Git_repos\\ENDG 511\\ENDG511_Final_Project\\audio_processing\\new_data\"\n",
    "file_name = \"german\"\n",
    "print(\"saved:    \")\n",
    "number = 1\n",
    "while True:\n",
    "    \n",
    "    array = record_audio_to_numpy()\n",
    "    image = numpy_to_fft(array)\n",
    "    # number = find_first_unused_garbage_number(GARBAGE_FOLDER, file_name)\n",
    "    image.save(f'{GARBAGE_FOLDER}\\\\{file_name}_{number}.png')\n",
    "    number +=1\n",
    "    image.save(f'temp.png')\n",
    "\n",
    "    print(\".\", end=\"\")\n",
    "    if (number % 10 == 0): print(\"\")\n",
    "    print(number, end=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hihihihih1None3\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\"*5, end=\"\")\n",
    "print(\"\\b1\", end = \"None\")\n",
    "print(len(\"121\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
