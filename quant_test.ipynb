{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37378f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## quantization test code - ENDG 511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7120832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pruned model size: 9.85 MB\n",
      "Quantized model size: 9.35 MB\n"
     ]
    }
   ],
   "source": [
    "# quantization (dynamic)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "import os\n",
    "\n",
    "# Load the pruned model (ensure it's on CPU for quantization)\n",
    "model_pruned = torch.load(r\"/Users/SeleemB/Desktop/ENDG511_Final_Project/models/model_language_mobilenet_20_epoch_new_normalize_0p6_dropout.pth\")\n",
    "model_pruned.eval()\n",
    "\n",
    "# Apply dynamic quantization to Linear layers\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model_pruned,                   # the model to quantize\n",
    "    {nn.Linear},                    # layers to quantize (Conv2d not supported here)\n",
    "    dtype=torch.qint8               # quantization data type to 8-bit int type\n",
    ")\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_model_path = \"models/quantized_iterative_pruned_model.pth\"\n",
    "torch.save(quantized_model, quantized_model_path)\n",
    "\n",
    "# Report model sizes\n",
    "original_size = os.path.getsize(\"/Users/SeleemB/Desktop/ENDG511_Final_Project/models/model_language_mobilenet_20_epoch_new_normalize.pth\") / 1e6\n",
    "quantized_size = os.path.getsize(quantized_model_path) / 1e6\n",
    "\n",
    "print(f\"Original pruned model size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b70f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class LanguageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for loading and processing spectrogram images of radio signals.\n",
    "\n",
    "    This dataset:\n",
    "    - Loads images from a specified directory.\n",
    "    - Applies preprocessing transformations (grayscale conversion, resizing, normalization).\n",
    "    - Computes class weights for handling class imbalance.\n",
    "    - Returns image tensors along with their respective labels.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Path to the dataset directory.\n",
    "    class_labels : list\n",
    "        List of signal class names.\n",
    "    class_weights : torch.Tensor\n",
    "        Normalized inverse frequency weights for each class.\n",
    "    data_files : list\n",
    "        List of all image file names in the dataset.\n",
    "    transform : torchvision.transforms.Compose\n",
    "        Transformations applied to each image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, augment=False):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading class names, computing class frequencies, and setting up transformations.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data_dir : str\n",
    "            Path to the dataset directory.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        # Define class labels (must match the dataset naming convention)\n",
    "        self.class_labels = [ 'arabic', 'english', 'german', 'mandarin', 'spanish', 'garbage', 'french']\n",
    "        #self.class_labels = [ 'arabic', 'german', 'mandarin', 'french']\n",
    "        self.random_crop = RandomCrop(size=(224, 224))  # assuming final size\n",
    "        self.augment = augment\n",
    "\n",
    "        # Get all filenames from the dataset directory\n",
    "        self.data_files = os.listdir(data_dir)\n",
    "\n",
    "        # Compute class frequencies (how many samples per class exist)\n",
    "        class_counts = {label: sum(1 for file in self.data_files if file.startswith(label)) for label in self.class_labels}\n",
    "\n",
    "        # Compute class weights (inverse frequency) to handle class imbalance\n",
    "        total_samples = sum(class_counts.values())\n",
    "        class_weights = [1 / (count / total_samples) if count > 0 else 0 for count in class_counts.values()]\n",
    "\n",
    "        # Normalize class weights so they sum to 1\n",
    "        class_weights_sum = sum(class_weights)\n",
    "        self.class_weights = torch.tensor([w / class_weights_sum for w in class_weights], dtype=torch.float)\n",
    "\n",
    "        # Define image transformations\n",
    "        self.transform = Compose([\n",
    "            ToTensor(),\n",
    "            Grayscale(num_output_channels=3),\n",
    "            Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "            RandomPerspective(distortion_scale=0.5, p=0.5),  # Perspective distortion\n",
    "            Normalize(mean=[-0.9256, -0.8168, -0.5910], std=[0.1704, 0.1742, 0.1734])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Loads an image, applies transformations, and returns it along with its label index.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        index : int\n",
    "            Index of the sample in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        tuple(torch.Tensor, torch.Tensor)\n",
    "            Transformed image tensor and its corresponding label index.\n",
    "        \"\"\"\n",
    "        # Get the filename of the sample\n",
    "        file_name = self.data_files[index]\n",
    "        file_path = os.path.join(self.data_dir, file_name)\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(file_path)\n",
    "\n",
    "        # Rotate 90 degrees (optional, remove if unnecessary)\n",
    "        image = image.transpose(Image.ROTATE_90)\n",
    "\n",
    "        # Ensure the image is in RGB mode (some formats might be grayscale)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        image = self.random_crop(image)\n",
    "        #image = self.transform(image)\n",
    "\n",
    "       # Convert image to spectrogram (numpy array) and apply augmentation\n",
    "        spectrogram = np.array(image)\n",
    "\n",
    "        if self.augment:\n",
    "            spectrogram = self.apply_spec_augment(spectrogram)\n",
    "\n",
    "        # Convert to tensor and apply transformations\n",
    "        image_tensor = self.transform(Image.fromarray(spectrogram))\n",
    "\n",
    "        if self.augment:\n",
    "            image_tensor = self.apply_noise(image_tensor)\n",
    "        # Extract the class label from the filename\n",
    "        class_label = file_name.split('_')[0]\n",
    "        sample_number = file_name.split('_')[1]\n",
    "        label_index = self.class_labels.index(class_label)\n",
    "\n",
    "        # Apply transformations and return image with label\n",
    "        return image_tensor, torch.tensor(label_index, dtype=torch.long), sample_number\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        int\n",
    "            Number of files in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data_files)\n",
    "    def apply_spec_augment(self, spectrogram):\n",
    "        \"\"\"Apply SpecAugment (time and frequency masking)\"\"\"\n",
    "        # Apply time and frequency masking using librosa\n",
    "        spectrogram = self.time_mask(spectrogram)\n",
    "        spectrogram = self.freq_mask(spectrogram)\n",
    "        return spectrogram\n",
    "\n",
    "    def time_mask(self, spectrogram, max_mask_size=50):\n",
    "        \"\"\"Apply time masking to the spectrogram\"\"\"\n",
    "        n_frames = spectrogram.shape[1]\n",
    "        mask_start = random.randint(0, n_frames - max_mask_size)\n",
    "        mask_end = mask_start + random.randint(1, max_mask_size)\n",
    "        spectrogram[:, mask_start:mask_end] = 0\n",
    "        return spectrogram\n",
    "\n",
    "    def freq_mask(self, spectrogram, max_mask_size=10):\n",
    "        \"\"\"Apply frequency masking to the spectrogram\"\"\"\n",
    "        n_freqs = spectrogram.shape[0]\n",
    "        mask_start = random.randint(0, n_freqs - max_mask_size)\n",
    "        mask_end = mask_start + random.randint(1, max_mask_size)\n",
    "        spectrogram[mask_start:mask_end, :] = 0\n",
    "        return spectrogram\n",
    "    \n",
    "    def apply_noise(self, img_tensor, noise_level=0.03):\n",
    "        noise = torch.randn_like(img_tensor) * noise_level\n",
    "        return torch.clamp(img_tensor + noise, 0.0, 1.0)  # Keep within valid image range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68be4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def stratified_split(dataset, train_test_split):\n",
    "    \"\"\"\n",
    "    Implement a Stratified Split for an imbalanced dataset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : Dataset\n",
    "        The dataset to split.\n",
    "    train_test_split : float\n",
    "        The proportion of data to allocate for training.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (train_dataset, test_dataset)\n",
    "        The stratified training and validation datasets.\n",
    "    \"\"\"\n",
    "    splitter = sklearn.model_selection.StratifiedShuffleSplit(train_size=train_test_split, random_state=None)\n",
    "\n",
    "    labels_iterable = [dataset[i][1] for i in range(len(dataset))]\n",
    "\n",
    "    for train_index, test_index in splitter.split(range(len(dataset)), labels_iterable):\n",
    "\n",
    "            train_dataset = [dataset[i] for i in train_index]\n",
    "            test_dataset = [dataset[i] for i in test_index]\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "    #raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adf0c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in /Users/SeleemB/Desktop/ENDG511_Final_Project/languages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m     14\u001b[0m ])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load validation dataset\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/SeleemB/Desktop/ENDG511_Final_Project/languages\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     18\u001b[0m data_loader_val \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# --------- Load Models ---------\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Pruned model path (original .pth)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/endg511_project/lib/python3.11/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    310\u001b[0m         root,\n\u001b[1;32m    311\u001b[0m         loader,\n\u001b[1;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[38;5;28;01mif\u001b[39;00m is_valid_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m    314\u001b[0m         target_transform\u001b[38;5;241m=\u001b[39mtarget_transform,\n\u001b[1;32m    315\u001b[0m         is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/miniconda3/envs/endg511_project/lib/python3.11/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/miniconda3/envs/endg511_project/lib/python3.11/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/miniconda3/envs/endg511_project/lib/python3.11/site-packages/torchvision/datasets/folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /Users/SeleemB/Desktop/ENDG511_Final_Project/languages."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --------- Load Validation Data ---------\n",
    "# Basic transform: resize to match model input, convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load validation dataset\n",
    "dataset = LanguageDataset(\"languages\")\n",
    "dataset_train, dataset_val = stratified_split(dataset, train_test_split=0.8)\n",
    "\n",
    "#set values as static values similar to LAB2_test_code.ipynb\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=16, num_workers=0,\n",
    "                             pin_memory=True, drop_last=False)\n",
    "\n",
    "# --------- Load Models ---------\n",
    "# Pruned model path (original .pth)\n",
    "pruned_model_path = \"/Users/SeleemB/Desktop/ENDG511_Final_Project/models/model_language_mobilenet_20_epoch_new_normalize_pruned.pth\"\n",
    "quantized_model_path = \"models/quantized_iterative_pruned_model.pth\"\n",
    "\n",
    "# Load models to CPU\n",
    "model_pruned = torch.load(pruned_model_path, map_location='cpu')\n",
    "model_pruned.eval()\n",
    "\n",
    "quantized_model = torch.load(quantized_model_path, map_location='cpu')\n",
    "quantized_model.eval()\n",
    "\n",
    "# --------- Evaluation Function ---------\n",
    "def evaluate_model(model, dataloader, max_batches=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_time = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to('cpu'), labels.to('cpu')\n",
    "\n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            end = time.time()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            total_time += (end - start)\n",
    "\n",
    "            if max_batches and i >= max_batches - 1:\n",
    "                break\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_inference_time = total_time / total\n",
    "    return accuracy, avg_inference_time\n",
    "\n",
    "# --------- Evaluate Models ---------\n",
    "print(\"Evaluating pruned model...\")\n",
    "acc_pruned, time_pruned = evaluate_model(model_pruned, data_loader_val, max_batches=50)\n",
    "\n",
    "print(\"Evaluating quantized model...\")\n",
    "acc_quant, time_quant = evaluate_model(quantized_model, data_loader_val, max_batches=50)\n",
    "\n",
    "# --------- Get Model Sizes ---------\n",
    "size_pruned = os.path.getsize(pruned_model_path) / 1e6\n",
    "size_quant = os.path.getsize(quantized_model_path) / 1e6\n",
    "\n",
    "# --------- Plot Comparison ---------\n",
    "labels = ['Accuracy (%)', 'Avg Inference Time (s)', 'Model Size (MB)']\n",
    "pruned_stats = [acc_pruned, time_pruned, size_pruned]\n",
    "quant_stats = [acc_quant, time_quant, size_quant]\n",
    "\n",
    "x = range(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar([p - width/2 for p in x], pruned_stats, width=width, label='Pruned Model')\n",
    "ax.bar([p + width/2 for p in x], quant_stats, width=width, label='Quantized Model')\n",
    "\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Model Comparison: Pruned vs Quantized')\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------- Print Final Stats ---------\n",
    "print(f\"\\n--- Evaluation Summary ---\")\n",
    "print(f\"Pruned Model     | Accuracy: {acc_pruned:.2f}% | Time: {time_pruned:.4f}s | Size: {size_pruned:.2f} MB\")\n",
    "print(f\"Quantized Model  | Accuracy: {acc_quant:.2f}% | Time: {time_quant:.4f}s | Size: {size_quant:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "endg511_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
